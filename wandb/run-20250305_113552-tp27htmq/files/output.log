  | Name      | Type              | Params | Mode
--------------------------------------------------------
0 | lstm      | LSTM              | 33.8 M | train
1 | fc        | Linear            | 129    | train
2 | criterion | BCEWithLogitsLoss | 0      | train
3 | accuracy  | BinaryAccuracy    | 0      | train
4 | auroc     | BinaryAUROC       | 0      | train
--------------------------------------------------------
33.8 M    Trainable params
0         Non-trainable params
33.8 M    Total params
135.013   Total estimated model params size (MB)
5         Modules in train mode
0         Modules in eval mode
/Users/luozisheng/miniconda3/envs/seg/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.
/Users/luozisheng/miniconda3/envs/seg/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (44) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
